{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI: github://langchain-ai/langchain-mcp-adapters/blob/main/README.md\n",
      "# LangChain MCP Adapters\n",
      "\n",
      "This library provides a lightweight wrapper that makes [Anthropic Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) tools compatible with [LangChain](https://github.com/langchain-ai/langchain) and [LangGraph](https://github.com/langchain-ai/langgraph).\n",
      "\n",
      "![MCP](static/img/mcp.png)\n",
      "\n",
      "> [!note]\n",
      "> A JavaScript/TypeScript version of this library is also available at [langchainjs](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mc\n"
     ]
    }
   ],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "for blob in resources:\n",
    "    print(\"URI:\", blob.metadata.get(\"uri\"))\n",
    "    print(blob.as_string()[:500])\n",
    "\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hereâ€™s a concise overview of the LangChain MCP Adapters library and how it fits into LangChain, LangGraph, and LangSmith.\n",
      "\n",
      "What is it\n",
      "- The LangChain MCP Adapters library provides a bridge between the Anthropic Model Context Protocol (MCP) tools ecosystem and LangChain/LangGraph.\n",
      "- It converts MCP tools into LangChain- and LangGraph-compatible tools, enabling agents to call and orchestrate MCP tools as if they were native LangChain tools.\n",
      "- It supports tools across multiple MCP servers, so an agent can pull and use tools from many servers at once.\n",
      "- It integrates the hundreds of MCP tool servers already published into LangGraph agents, simplifying multi-server tool integration.\n",
      "\n",
      "Key features and whatâ€™s new (based on recent releases)\n",
      "- Multiserver support: Pull tools from multiple MCP servers and use them together in a single agent.\n",
      "- Multimodal tool support: Tools can accept/produce images, text, and other modalities.\n",
      "- Elicitation and clarifications: Easy support for prompt-driven clarifications and multi-turn interactions with MCP tools.\n",
      "- Structured tool outputs as artifacts: Tool results can be stored in a structured, inspectable format for easier downstream processing.\n",
      "- Tool name prefixes for multi-server setups: Helps avoid naming collisions when using tools from several MCP servers.\n",
      "- Documentation updates: There are dedicated docs showing how to use MCP with LangChain (OSS docs), plus changelog notes for MCP Adapters releases.\n",
      "\n",
      "Where it fits in the ecosystem\n",
      "- MCP (Model Context Protocol) is a standard for exposing tools and context to LLMs. MCP Adapters let LangChain/LangGraph agents use MCP-defined tools as if they were native tools.\n",
      "- The library is designed to work seamlessly with LangChain agents (and LangGraph) to orchestrate tools across one or more MCP servers.\n",
      "\n",
      "Language/implementation variants\n",
      "- Python: The main LangChain MCP Adapters library is available as langchain-mcp-adapters (GitHub: langchain-ai/langchain-mcp-adapters). It provides a client (MultiServerMCPClient) to connect to MCP servers and fetch tools, which you can then feed into LangChain agents.\n",
      "- JavaScript/TypeScript: There is a JavaScript/TypeScript variant (langchainjs-mcp-adapters) for LangChain.js, which provides similar functionality for the JS ecosystem. The repo notes indicate that the JS repo has moved/merged in some places, and there are both Python and JS examples in the ecosystem.\n",
      "\n",
      "Where to learn more and get started\n",
      "- Python usage and API surface: langchain-ai/langchain-mcp-adapters on GitHub. Typical workflow involves creating a MultiServerMCPClient, loading tools via get_tools, and passing them to a LangChain agent.\n",
      "- JavaScript/TypeScript usage: langchainjs-mcp-adapters on GitHub (LangChain.js MCP adapters).\n",
      "- Official docs (LangChain): Model Context Protocol (MCP) page and the MCP docs, which describe how to connect to MCP servers, get tools, and use them with agents (including lifecycle management and elicitation).\n",
      "- Changelogs and announcements: LangChain changelog entries about â€œMCP Adapters for LangChain and LangGraphâ€ and the 0.2.0 release, which highlight new features like multimodal support, elicitation, structured artifacts, and per-server tool naming.\n",
      "\n",
      "Quick-start sketch (Python)\n",
      "- Create an MCP client that points at your MCP servers\n",
      "  - client = MultiServerMCPClient({ mcpServers: { math: { transport: \"stdio\", command: \"python\", args: [\"path/to/math_server.py\"] }, weather: { url: \"https://example.com/weather/mcp\" } } })\n",
      "- Load tools from servers\n",
      "  - tools = await client.get_tools()\n",
      "- Create a LangChain agent with those tools\n",
      "  - agent = create_agent(llm, tools)\n",
      "- Invoke the agent\n",
      "  - response = await agent.ainvoke({ \"input\": \"What's the weather in NYC and compute (3+5)*2?\" })\n",
      "\n",
      "If youâ€™re interested, I can tailor a quick-start guide for you (Python or JavaScript) with concrete code snippets and a minimal MCP server example. Do you want:\n",
      "- a Python-based quick-start walkthrough, or\n",
      "- a JavaScript/LangChain.js quick-start walkthrough?\n",
      "\n",
      "Would you like me to pull up the latest docs pages or point you to a specific repository or example suite?\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "#pprint(response)\n",
    "print(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36695a3f-0fb3-4a2d-b178-28051d40b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 HumanMessage\n",
      "1 AIMessage\n",
      "  tool_calls: [{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_Q1CZJbgK9zChO8lo6UATYqUs', 'type': 'tool_call'}]\n",
      "2 ToolMessage\n",
      "  tool output: [{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999956,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"import{createAgent} from \\\\\"langchain\\\\\"; import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\";// Create client and connect to server const client = new MultiServerMCPClient({// Global tool configuration options// Whether to throw on errors if a tool fails to load (optional, default: true) throwOnLoadError true,// Whether to prefix tool names with the server name (optional, default: false) prefixToolNameWithServerName false,// Optional additional prefix for tool names (optional, default: \\\\\"\\\\\") additionalToolNamePrefix \\\\\"\\\\\",// Use standardized content block format in tool outputs useStandardContentBlocks true,// Behavior when a server fails to connect: \\\\\"throw\\\\\" (default) or \\\\\"ignore\\\\\" onConnectionError \\\\\"ignore\\\\\",// Server configuration mcpServers{// adds a STDIO connection to a server named \\\\\"math\\\\\" math{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-math\\\\\"],// Restart configuration for stdio transport restart{enabled true, maxAttempts 3, delayMs 1000,},},// here\\'s a filesystem server filesystem{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-filesystem\\\\\"],},// Sreamable HTTP transport example, with auth headers and automatic SSE fallback disabled (defaults to enabled) weather{url\\\\\"https://example.com/weather/mcp\\\\\", headers{Authorization \\\\\"Bearer token123\\\\\",} automaticSSEFallback false},// OAuth 2.0 authentication (recommended for secure servers)\\\\\"oauth-protected-server\\\\\"{url\\\\\"https://protected.example.com/mcp\\\\\", authProvider new MyOAuthProvider({// Your OAuth provider implementation redirectUrl\\\\\"https://myapp.com/oauth/callback\\\\\", clientMetadata{redirect_uris[\\\\\"https://myapp.com/oauth/callback\\\\\"], client_name \\\\\"My MCP Client\\\\\", scope\\\\\"mcp:read mcp:write\\\\\"}}),// Can still include custom headers for non-auth purposes headers{\\\\\"User-Agent\\\\\"\\\\\"My-MCP-Client/1.0\\\\\"}},// how to force SSE, for old servers that are known to only support SSE (streamable HTTP falls back automatically if unsure) github{transport \\\\\"sse\\\\\",// also works with \\\\\"type\\\\\" field instead of \\\\\"transport\\\\\" url\\\\\"https://example.com/mcp\\\\\", reconnect{enabled true, maxAttempts 5, delayMs 2000,},},},}); const tools = await client.\",\\n      \"score\": 0.99997115,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/langchain-mcp-adapters-0-2-0\",\\n      \"title\": \"LangChain MCP Adapters 0.2.0\",\\n      \"content\": \"LangChain - Changelog | LangChain MCP Adapters 0.2.0. Sign up for our newsletter to stay up to date. LangChain MCP Adapters 0.2.0. Image 1**LangChain MCP Adapters 0.2.0 is live.**This release brings quality-of-life upgrades for anyone building with MCP tools in LangChain:. Image 2**Multimodal tool support**. Use tools that accept and produce images, text, and other modalitiesâ€”powered by LangChainâ€™s standard content blocks. Image 3**Elicitation support via callbacks**. Easily implement prompt-driven clarifications and multi-turn tool interactions. Image 4**Structured tool output as artifacts**. Tool results now store structured content as artifacts, making downstream processing and inspection much cleaner. Image 5**Tool name prefixes for multi-server setups**. Eliminate naming collisions and run multiple MCP servers seamlessly. We\\'ve also released new docs to help you get started fast: https://docs.langchain.com/oss/python/langchain/mcp. Technical release notes: https://github.com/langchain-ai/langchain-mcp-adapters/releases/tag/langchain-mcp-adapters%3D%3D0.2.0. ##### Subscribe to updates. Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions. reCAPTCHA privacy and terms apply.\",\\n      \"score\": 0.9999535,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters\",\\n      \"title\": \"LangChain MCP Integration: Complete Guide to MCP Adapters\",\\n      \"content\": \"from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\\\\\"python\\\\\", \\\\\"database_mcp_server.py\\\\\"], transport_type=\\\\\"stdio\\\\\", environment={ \\\\\"DATABASE_URL\\\\\": \\\\\"postgresql://user:pass@localhost:5432/mydb\\\\\", \\\\\"MAX_CONNECTIONS\\\\\": \\\\\"10\\\\\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\\\\\"gpt-4\\\\\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Find all customers who made purchases over $500 in the last month\\\\\" }). import os # REST API MCP server integration api_adapter = MCPAdapter( url=\\\\\"http://localhost:3000/mcp\\\\\", transport_type=\\\\\"sse\\\\\", headers={ \\\\\"Authorization\\\\\": f\\\\\"Bearer {os.getenv(\\'API_TOKEN\\')}\\\\\", \\\\\"User-Agent\\\\\": \\\\\"LangChain-MCP-Client/1.0\\\\\" } ) api_tools = await api_adapter.get_tools() crm_agent = create_react_agent(llm, api_tools) # Use the agent to interact with the CRM API customer_data = await crm_agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Create a new lead for John Smith with email [email\\xa0protected]\\\\\" }). Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows.\",\\n      \"score\": 0.999944,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers. To test your agent with MCP tool servers, use the following examples:. If you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`. Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:. MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. [Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution.\",\\n      \"score\": 0.99993896,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.0,\\n  \"request_id\": \"0adc51e7-fe73-4e4c-b7ef-4e300cc8f5d6\"\\n}', 'id': 'lc_d0a7e36b-0c08-4733-9c82-f02f44097a4e'}]\n",
      "3 AIMessage\n",
      "  tool_calls: [{'name': 'search_web', 'args': {'query': 'github langchain-mcp-adapters'}, 'id': 'call_T1hG0iHUPbmn9clat9CLhnQA', 'type': 'tool_call'}]\n",
      "4 ToolMessage\n",
      "  tool output: [{'type': 'text', 'text': '{\\n  \"query\": \"github langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchainjs-mcp-adapters\",\\n      \"title\": \"GitHub - langchain-ai/langchainjs-mcp-adapters: ** THIS ...\",\\n      \"content\": \"# Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. You switched accounts on another tab or window. langchain-ai   /  **langchainjs-mcp-adapters**  Public archive. \\\\\\\\*\\\\\\\\* THIS REPO HAS MOVED TO  \\\\\\\\*\\\\\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. 245 stars   34 forks   Branches   Tags   Activity. # langchain-ai/langchainjs-mcp-adapters. | RELEASE\\\\\\\\_NOTES.md | RELEASE\\\\\\\\_NOTES.md |  |  |. | tsconfig.cjs.json | tsconfig.cjs.json |  |  |. | tsconfig.examples.json | tsconfig.examples.json |  |  |. | tsconfig.tests.json | tsconfig.tests.json |  |  |. ## Repository files navigation. # LangChain.js MCP Adapters. This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js. \\\\\\\\*\\\\\\\\* THIS REPO HAS MOVED TO  \\\\\\\\*\\\\\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. javascript   typescript   mcp   ai-tools   langchain   llm-tools   openai-functions   langchainjs   llm-agents   agent-tools   llm-integration   model-context-protocol.\",\\n      \"score\": 0.8875269,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers. To test your agent with MCP tool servers, use the following examples:. If you need to control the [lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`. Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:. MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. [Elicitation](https://modelcontextprotocol.io/specification/2025-11-25/client/elicitation#elicitation) allows MCP servers to request additional input from users during tool execution.\",\\n      \"score\": 0.8719229,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain ðŸ”Œ MCP\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.8332299,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.82492816,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/box-community/langchain-box-mcp-adapter\",\\n      \"title\": \"box-community/langchain-box-mcp-adapter\",\\n      \"content\": \"box-community   /  **langchain-box-mcp-adapter**  Public. This sample implements the Langchain MCP adapter to the Box MCP server. 1 star   1 fork   Branches   Tags   Activity. # box-community/langchain-box-mcp-adapter. ## Repository files navigation. # langchain-box-mcp-adapter. This sample project implements the Langchain MCP adapter to the Box MCP server. It demonstrates how to integrate Langchain with a Box MCP server using tools and agents. * **Tool Loading**: Dynamically loads tools from the MCP server. + `langchain-mcp-adapters>=0.0.8`. 1. Ensure the MCP server is set up and accessible at the specified path in the project. 2. Update the StdioServerParameters in src/simple\\\\\\\\_client.py or src/graph.py with the correct path to your MCP server script. server_params = StdioServerParameters command = \\\\\"uv\\\\\" args =\\\\\"--directory\\\\\"\\\\\"/your/absolute/path/to/the/mcp/server/mcp-server-box\\\\\" \\\\\"run\\\\\"\\\\\"src/mcp_server_box.py\\\\\". uv run src/simple_client.py. The graph-based agent can be used by invoking the make\\\\\\\\_graph function in src/graph.py. * src/graph.py: Contains the graph-based agent setup. See the LICENSE file for details. This sample implements the Langchain MCP adapter to the Box MCP server.\",\\n      \"score\": 0.8245895,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.68,\\n  \"request_id\": \"d23489ec-8402-4efc-afb5-63d9ba79465e\"\\n}', 'id': 'lc_c3ea6a00-847a-4d55-bd09-c71570d32b68'}]\n",
      "5 AIMessage\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage, AIMessage\n",
    "\n",
    "for i, m in enumerate(response[\"messages\"]):\n",
    "    print(i, type(m).__name__)\n",
    "    if isinstance(m, AIMessage) and getattr(m, \"tool_calls\", None):\n",
    "        print(\"  tool_calls:\", m.tool_calls)\n",
    "    if isinstance(m, ToolMessage):\n",
    "        print(\"  tool output:\", m.content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3062982f-1503-4774-8a9a-67bcc130e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def pretty_tool_messages(messages, max_chars=800):\n",
    "    for i, m in enumerate(messages):\n",
    "        if not isinstance(m, ToolMessage):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- ToolMessage @ index {i} ---\")\n",
    "        print(f\"tool_call_id: {getattr(m, 'tool_call_id', None)}\")\n",
    "\n",
    "        content = m.content\n",
    "\n",
    "        # Tool outputs are often: list[{\"type\":\"text\",\"text\":\"{...json...}\", ...}]\n",
    "        if isinstance(content, list):\n",
    "            for j, item in enumerate(content):\n",
    "                print(f\"\\n  [block {j}] type={item.get('type')}\")\n",
    "                txt = item.get(\"text\", \"\")\n",
    "\n",
    "                # If the text itself is JSON, parse it\n",
    "                try:\n",
    "                    obj = json.loads(txt)\n",
    "                    # Show the most useful bits first\n",
    "                    print(\"  query:\", obj.get(\"query\"))\n",
    "                    results = obj.get(\"results\", [])\n",
    "                    print(f\"  results: {len(results)}\")\n",
    "                    for k, r in enumerate(results[:5], start=1):\n",
    "                        print(f\"    {k}. {r.get('title')}\")\n",
    "                        print(f\"       {r.get('url')}\")\n",
    "                        snippet = (r.get(\"content\") or \"\")[:200].replace(\"\\n\", \" \")\n",
    "                        print(f\"       snippet: {snippet}...\")\n",
    "                except Exception:\n",
    "                    # Not JSON; print raw (truncated)\n",
    "                    print(txt[:max_chars] + (\"...\" if len(txt) > max_chars else \"\"))\n",
    "        else:\n",
    "            # If it's already a dict/str\n",
    "            pprint(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35f4a16-1171-4cc6-8e91-8edea5d58e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ToolMessage @ index 2 ---\n",
      "tool_call_id: call_Q1CZJbgK9zChO8lo6UATYqUs\n",
      "\n",
      "  [block 0] type=text\n",
      "  query: langchain-mcp-adapters\n",
      "  results: 5\n",
      "    1. MCP Adapters for LangChain and LangGraph\n",
      "       https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\n",
      "       snippet: # LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Mode...\n",
      "    2. langchain/mcp-adapters - NPM\n",
      "       https://www.npmjs.com/package/@langchain/mcp-adapters\n",
      "       snippet: import{createAgent} from \"langchain\"; import{ChatOpenAI} from\"@langchain/openai\"; import{MultiServerMCPClient} from\"@langchain/mcp-adapters\";// Create client and connect to server const client = new M...\n",
      "    3. LangChain MCP Adapters 0.2.0\n",
      "       https://changelog.langchain.com/announcements/langchain-mcp-adapters-0-2-0\n",
      "       snippet: LangChain - Changelog | LangChain MCP Adapters 0.2.0. Sign up for our newsletter to stay up to date. LangChain MCP Adapters 0.2.0. Image 1**LangChain MCP Adapters 0.2.0 is live.**This release brings q...\n",
      "    4. LangChain MCP Integration: Complete Guide to MCP Adapters\n",
      "       https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters\n",
      "       snippet: from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_comm...\n",
      "    5. Model Context Protocol (MCP) - Docs by LangChain\n",
      "       https://docs.langchain.com/oss/python/langchain/mcp\n",
      "       snippet: [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools de...\n",
      "\n",
      "--- ToolMessage @ index 4 ---\n",
      "tool_call_id: call_T1hG0iHUPbmn9clat9CLhnQA\n",
      "\n",
      "  [block 0] type=text\n",
      "  query: github langchain-mcp-adapters\n",
      "  results: 5\n",
      "    1. GitHub - langchain-ai/langchainjs-mcp-adapters: ** THIS ...\n",
      "       https://github.com/langchain-ai/langchainjs-mcp-adapters\n",
      "       snippet: # Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. You switched accounts on another tab or window. langchai...\n",
      "    2. Model Context Protocol (MCP) - Docs by LangChain\n",
      "       https://docs.langchain.com/oss/python/langchain/mcp\n",
      "       snippet: [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools de...\n",
      "    3. langchain-ai/langchain-mcp-adapters: LangChain ðŸ”Œ MCP\n",
      "       https://github.com/langchain-ai/langchain-mcp-adapters\n",
      "       snippet: from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolu...\n",
      "    4. MCP Adapters for LangChain and LangGraph\n",
      "       https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\n",
      "       snippet: # LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Mode...\n",
      "    5. box-community/langchain-box-mcp-adapter\n",
      "       https://github.com/box-community/langchain-box-mcp-adapter\n",
      "       snippet: box-community   /  **langchain-box-mcp-adapter**  Public. This sample implements the Langchain MCP adapter to the Box MCP server. 1 star   1 fork   Branches   Tags   Activity. # box-community/langchai...\n"
     ]
    }
   ],
   "source": [
    "pretty_tool_messages(response[\"messages\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/Chicago\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='f475e414-c690-471d-901f-7449a19d8147'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 218, 'prompt_tokens': 293, 'total_tokens': 511, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpbBQ8SMG3rt02Hv1A0PrCDffLPsq', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b4675-f867-7f32-a5ed-daa4ffcc4184-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/Chicago'}, 'id': 'call_mCdWmlpluemvpJroI4NJyisk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 293, 'output_tokens': 218, 'total_tokens': 511, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/Chicago\",\\n  \"datetime\": \"2025-12-22T08:28:18-06:00\",\\n  \"day_of_week\": \"Monday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_9ee58adc-0079-46d8-b015-dce1453e7a8b'}], name='get_current_time', id='d3887fb1-169d-4e13-a61f-cd97516de827', tool_call_id='call_mCdWmlpluemvpJroI4NJyisk'),\n",
      "              AIMessage(content='Current time in Chicago (Central Time): Monday, December 22, 2025, 08:28 AM CST (UTC-6). \\n\\nWant this in another timezone? I can convert it for you.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 436, 'prompt_tokens': 374, 'total_tokens': 810, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpbBTA9SPMhwQlTWFCbJmhithhQ4l', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4676-0769-7060-b1db-6353c356da52-0', usage_metadata={'input_tokens': 374, 'output_tokens': 436, 'total_tokens': 810, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time in Chicago (Central Time): Monday, December 22, 2025, 08:28 AM CST (UTC-6). \n",
      "\n",
      "Want this in another timezone? I can convert it for you.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d50083-5cfb-4fbc-a04d-d2df32f2741a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
